% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fam_stackProb.R
\name{fam_stackProb}
\alias{fam_stackProb}
\title{Family for probabilistic additive stacking}
\usage{
fam_stackProb(logP, ridgePen = 1e-05)
}
\arguments{
\item{logP}{n x K matrix of the log-predictive densities of the experts.
n is the total number of available observations, while K is the number of experts.
Hence, the i-th columns contains the log-densities corresponding to the k-th expert.}

\item{ridgePen}{small ridge penalty on regression coefficients, useful for numerical stability.
Do not change it unless you know what you are doing.}
}
\value{
A family that can be fitted via mgcv::gam.
}
\description{
Creates a family to be used in mgcv::gam for probabilistic additive stacking.
}
\examples{
library(gamFactory)

#### Very basic example: fitting a mixture with known mixture members (experts)
# Generate some data from a weighted mixture of normals
# The mixture weights change with x
set.seed(424)
n <- 1e3
x <- seq(0, pi, length.out = n)
w <- sin(x)

ii <- rbinom(n, size = 1, prob = w) 
y <- ii * rnorm(n, 0, 2) + (!ii) * rnorm(n, 4, 0.5)
plot(x, y)

# Normally our two experts would be fitted to some training data
# but here they are known and they are two normals N(0, 2) and N(4, 0.5).
# We evaluate their log-densities on the stacking data
logDen <- cbind(dnorm(y, 0, 2, log = TRUE), 
                dnorm(y, 4, 0.5, log = TRUE))

# Estimate the mixture weights via additive stacking
fit <- gam(list(y ~ s(x)), 
           family = fam_stackProb(logDen), 
           data = data.frame(x=x, y=y))

# Compare estimated vs true weight of first expert or mixture component
prW <- predict(fit, type = "response", se = TRUE)
plot(x, prW$fit[ , 1], type = 'l', 
     main = "Estimated (black) and true (red) weight", 
     ylab = "Weight of 1st expert")
lines(x, prW$fit[ , 1] + 2*prW$se.fit[ , 1], lty = 2)
lines(x, prW$fit[ , 1] - 2*prW$se.fit[ , 1], lty = 2 )
lines(x, w, col = 2) # TRUTH


#### More realistic example: 
# We fit a simple and a more complex GAM model to several training data sets
# of increasing size. Then we simulate stacking data sets of the same sizes
# and we use them to estimate the weights of the two GAM models in the 
# stacking ensemble (a mixture of the two GAMs). We let the stacking weights
# depend on the sample size, because we expect that the more complex GAM should
# be give more weight for large data sets (because it has less bias).

# Generating training and stacking sets of increasing size:
# 60 data sets of size 100, 125 sets of size 20, ....
sizes <- c(rep(100, 60), rep(125, 20), rep(150, 5), 
           rep(175, 5), rep(200, 5), rep(300, 5),
           rep(400, 5), rep(600, 5), rep(800, 5))

ns <- length(sizes)

# Simulate training data sets from a standard GAM example 
datListTrain <- lapply(sizes, 
                       function( .n ){
                         gamSim(1, n = .n, dist = "binary", 
                                scale = 0.33, verbose = FALSE)
                       })
datTrain <- do.call("rbind", datListTrain)
datTrain <- as.data.frame(datTrain)
datTrain$sizes <- rep(sizes, sizes)

# Simulate stacking data sets in the same way
datListStack <- lapply(sizes, 
                       function( .n ){
                         gamSim(1, n = .n, dist = "binary", 
                                scale = 0.33, verbose = FALSE)
                       })
datStack <- do.call("rbind", datListStack)
datStack <- as.data.frame(datStack)
datStack$sizes <- rep(sizes, sizes)

# Estimate simple GAM on each training data set and 
# evaluate log-density (log-likelihood) of predictions on stacking sets
m1 <- lapply(1:ns, 
             function(.kk){
               .fit <- gam(y ~ s(x0, k = 3) + s(x1, k = 3) + 
                               s(x2, k = 3) + s(x3, k = 3), 
                           family=binomial, data = datListTrain[[.kk]], 
                           method="REML")
               .y <- datListStack[[.kk]]$y
               .p <- predict(.fit, newdata = datListStack[[.kk]], 
                             type = "response")
               .stack <- log(.p) * .y + log1p( - .p ) * ( !.y )
               return( list("fit" = .fit, "stack" = .stack) )
             })

# Estimate complex GAM on each training data set and 
# evaluate log-density (log-likelihood) of predictions on stacking sets
m2 <- lapply(1:ns, 
             function(.kk){
               .fit <- gam(y ~ s(x0, k = 10) + s(x1, k = 10) +
                               s(x2, k = 10) + s(x3, k = 10), 
                           family=binomial, data = datListTrain[[.kk]], 
                           method="REML")
               .y <- datListStack[[.kk]]$y
               .p <- predict(.fit, newdata = datListStack[[.kk]], 
                             type = "response")
               .stack <- log(.p) * .y + log1p( - .p ) * ( !.y )
               return( list("fit" = .fit, "stack" = .stack) )
             })

# Build matrix of log-densities on stacking set for both sets of GAMs
logPStack <- cbind(do.call("c", lapply(m1, "[[", "stack")), 
                   do.call("c", lapply(m2, "[[", "stack")))

# Fit additive stacking model where the experts weights depends on the 
# sample sizes 
fitStack <- gam(list(y ~ s(log(sizes), k = 7)), 
                 family = fam_stackProb(logP = logPStack), 
                 data = datStack)

# As expected, the weight of the second model (the more complex GAM) 
# increases with the size of the training set
prW <- predict(fitStack, type = "response", se = TRUE)
plot(datStack$sizes, prW$fit[ , 2], type = 'l', 
     main = "Estimated (black) and true (red) weight", 
     ylab = "Weight of 2nd expert", ylim = c(0.2, 1.1))
lines(datStack$sizes, prW$fit[ , 2] + 2*prW$se.fit[ , 2], lty = 2)
lines(datStack$sizes, prW$fit[ , 2] - 2*prW$se.fit[ , 2], lty = 2 )

}
